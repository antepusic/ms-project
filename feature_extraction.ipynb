{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from scipy.signal import resample\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "CLEAN_DATA_DIR = \"clean_data\"\n",
    "FINAL_DATA_DIR = \"final_data\"\n",
    "\n",
    "DOMAINS = (\"temp\", \"hr\", \"gsr\", \"rr\")\n",
    "\n",
    "TEMP = 0\n",
    "HR = 1\n",
    "GSR = 2\n",
    "RR = 3\n",
    "\n",
    "L_DOMAIN = 30"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "def extract_stat_features(segments: pd.DataFrame, domain: str) -> pd.DataFrame:\n",
    "    basic_features: List[str] = [\n",
    "        \"mean\",\n",
    "        \"std\",\n",
    "        \"skew\",\n",
    "        \"kurtosis\",\n",
    "        \"diff\",\n",
    "        \"diff2\",\n",
    "        \"q25\",\n",
    "        \"q75\",\n",
    "        \"qdev\",\n",
    "        \"max-min\",\n",
    "    ]\n",
    "\n",
    "    feature_names: List[str] = [f\"{domain}_{feature}\" for feature in basic_features]\n",
    "\n",
    "    values: np.ndarray = np.column_stack(\n",
    "        [\n",
    "            segments.mean(axis=1).values,  # mean\n",
    "            segments.std(axis=1).values,  # standard deviation\n",
    "            segments.skew(axis=1).values,  # skewness\n",
    "            segments.kurtosis(axis=1).values,  # kurtosis\n",
    "            segments.diff(axis=1).mean(axis=1).values,  # 1st derivative mean\n",
    "            segments.diff(axis=1)\n",
    "            .diff(axis=1)\n",
    "            .mean(axis=1)\n",
    "            .values,  # 2nd derivative mean\n",
    "            segments.quantile(0.25, axis=1).values,  # 25th quantile\n",
    "            segments.quantile(0.75, axis=1).values,  # 75th quantile\n",
    "            segments.quantile(0.75, axis=1).values\n",
    "            - segments.quantile(0.25, axis=1).values,  # quartile deviation\n",
    "            segments.max(axis=1).values - segments.min(axis=1).values,  # range\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame(values, columns=feature_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "def split_domains(\n",
    "    segments: pd.DataFrame,\n",
    ") -> List[pd.DataFrame]:\n",
    "    # order = [\"temp\", \"hr\", \"gsr\", \"rr\"]\n",
    "    domain_indices: List[int] = [0, 30, 60, 90]\n",
    "\n",
    "    return [segments.iloc[:, i : i + L_DOMAIN] for i in domain_indices]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "def extract_eda_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    feature_keys = [\n",
    "        \"SCR_Onsets\",\n",
    "        \"SCR_Peaks\",\n",
    "        \"SCR_Height\",\n",
    "        \"SCR_Amplitude\",\n",
    "        \"SCR_RiseTime\",\n",
    "        \"SCR_Recovery\",\n",
    "        \"SCR_RecoveryTime\",\n",
    "    ]\n",
    "\n",
    "    # for each feature key we will calculate min, max and mean values\n",
    "    feature_names = []\n",
    "    for f in feature_keys:\n",
    "        feature_names.append(f\"min_{f}\")\n",
    "        feature_names.append(f\"max_{f}\")\n",
    "        feature_names.append(f\"mean_{f}\")\n",
    "\n",
    "    # iterate through all 30-second segments\n",
    "    features_arr = []\n",
    "    for i in range(len(df)):\n",
    "        my_eda = df.iloc[i].dropna()\n",
    "        my_eda_resampled = resample(\n",
    "            my_eda.values, len(my_eda.values) * 10\n",
    "        )  # upsampling (neurokit requires 10Hz sampling frequency)\n",
    "        signals, info = nk.eda_process(my_eda_resampled, sampling_rate=10)\n",
    "\n",
    "        segment_features = []\n",
    "        for k in feature_keys:\n",
    "            feature_min = 0\n",
    "            feature_max = 0\n",
    "            feature_mean = 0\n",
    "\n",
    "            values = info[k]\n",
    "            values = values[~np.isnan(values)]\n",
    "            if (\n",
    "                len(values) > 0\n",
    "            ):  # update feature-values if there is at least 1 detected value (e.g., at least one peak), else leave 0\n",
    "                feature_min = np.min(values)\n",
    "                feature_max = np.max(values)\n",
    "                feature_mean = np.mean(values)\n",
    "            segment_features.extend([feature_min, feature_max, feature_mean])\n",
    "        features_arr.append(segment_features)\n",
    "\n",
    "    return pd.DataFrame(features_arr, columns=feature_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "def extract_hrv_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    feature_names = [\n",
    "        \"HRV_RMSSD\",\n",
    "        \"HRV_MeanNN\",\n",
    "        \"HRV_SDNN\",\n",
    "        \"HRV_SDSD\",\n",
    "        \"HRV_CVNN\",\n",
    "        \"HRV_CVSD\",\n",
    "        \"HRV_MedianNN\",\n",
    "        \"HRV_MadNN\",\n",
    "        \"HRV_MCVNN\",\n",
    "        \"HRV_IQRNN\",\n",
    "        \"HRV_pNN50\",\n",
    "        \"HRV_pNN20\",\n",
    "        \"HRV_TINN\",\n",
    "        \"HRV_HTI\",\n",
    "        \"HRV_ULF\",\n",
    "        \"HRV_VLF\",\n",
    "        \"HRV_LF\",\n",
    "        \"HRV_HF\",\n",
    "        \"HRV_VHF\",\n",
    "        \"HRV_LFHF\",\n",
    "        \"HRV_LFn\",\n",
    "        \"HRV_HFn\",\n",
    "        \"HRV_LnHF\",\n",
    "        \"HRV_SD1\",\n",
    "        \"HRV_SD2\",\n",
    "        \"HRV_SD1SD2\",\n",
    "        \"HRV_S\",\n",
    "        \"HRV_CSI\",\n",
    "        \"HRV_CVI\",\n",
    "        \"HRV_CSI_Modified\",\n",
    "        \"HRV_PIP\",\n",
    "        \"HRV_IALS\",\n",
    "        \"HRV_PSS\",\n",
    "        \"HRV_PAS\",\n",
    "        \"HRV_GI\",\n",
    "        \"HRV_SI\",\n",
    "        \"HRV_AI\",\n",
    "        \"HRV_PI\",\n",
    "        \"HRV_C1d\",\n",
    "        \"HRV_C1a\",\n",
    "        \"HRV_SD1d\",\n",
    "        \"HRV_SD1a\",\n",
    "        \"HRV_C2d\",\n",
    "        \"HRV_C2a\",\n",
    "        \"HRV_SD2d\",\n",
    "        \"HRV_SD2a\",\n",
    "        \"HRV_Cd\",\n",
    "        \"HRV_Ca\",\n",
    "        \"HRV_SDNNd\",\n",
    "        \"HRV_SDNNa\",\n",
    "        \"HRV_ApEn\",\n",
    "        \"HRV_SampEn\",\n",
    "        \"HRV_MSE\",\n",
    "        \"HRV_CMSE\",\n",
    "        \"HRV_RCMSE\",\n",
    "        \"HRV_DFA\",\n",
    "        \"HRV_CorrDim\",\n",
    "    ]\n",
    "\n",
    "    features_arr = []\n",
    "    for i in range(len(df)):\n",
    "        # noinspection PyBroadException\n",
    "        try:\n",
    "            rr = df.iloc[i].dropna()  # 30-second RR intervals\n",
    "\n",
    "            # convert RR intervals to peaks array (input expected by neurokit)\n",
    "            peaks_rr = np.zeros((len(rr) + 1) * 1000)\n",
    "            peaks_rr[0] = 1\n",
    "            prev_peak = 0\n",
    "            for r in rr:\n",
    "                peak_idx = prev_peak + int(r * 1000)\n",
    "                prev_peak = peak_idx\n",
    "                peaks_rr[peak_idx] = 1\n",
    "\n",
    "            segment_features = nk.hrv(peaks_rr, sampling_rate=1000, show=False)\n",
    "            features_arr.append(segment_features)\n",
    "        except Exception:\n",
    "            values = np.zeros(len(feature_names))\n",
    "            segment_features = pd.DataFrame([values], columns=feature_names)\n",
    "            features_arr.append(segment_features)\n",
    "\n",
    "    return pd.concat(features_arr, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "def select_hrv_features(file_paths: List[str]) -> List[pd.DataFrame]:\n",
    "    rr_segments_per_user: pd.DataFrame = pd.concat(\n",
    "        [\n",
    "            pd.read_csv(file_path)\n",
    "            .drop(\"Unnamed: 0\", axis=1)\n",
    "            .iloc[:, 90:120]\n",
    "            .rolling(3, axis=1)\n",
    "            .mean()\n",
    "            for file_path in file_paths\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    lengths: List[int] = [len(pd.read_csv(file_path)) for file_path in file_paths]\n",
    "\n",
    "    hrv_features_per_user: pd.DataFrame = extract_hrv_features(rr_segments_per_user)\n",
    "    hrv_features_per_user.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    good_features = hrv_features_per_user.isnull().sum() == 0\n",
    "    hrv_features_per_user = hrv_features_per_user[\n",
    "        hrv_features_per_user.columns[good_features]\n",
    "    ]\n",
    "\n",
    "    hrv_features: List[pd.DataFrame] = []\n",
    "    i = 0\n",
    "    for l in lengths:\n",
    "        hrv_features.append(hrv_features_per_user.iloc[i : i + l, :].reset_index())\n",
    "        i += l\n",
    "\n",
    "    return hrv_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "def extract_features(source_data_dir: str) -> List[pd.DataFrame]:\n",
    "    file_names: List[str] = sorted(\n",
    "        [\n",
    "            file_name\n",
    "            for file_name in os.listdir(source_data_dir)\n",
    "            if \"segments\" in file_name\n",
    "        ]\n",
    "    )\n",
    "    file_paths: List[str] = [\n",
    "        f\"{source_data_dir}/{file_name}\" for file_name in file_names\n",
    "    ]\n",
    "\n",
    "    hrv_features_per_user: List[pd.DataFrame] = select_hrv_features(\n",
    "        file_paths=file_paths\n",
    "    )\n",
    "\n",
    "    all_features_per_user: List[pd.DataFrame] = []\n",
    "    for file_path, hrv_features in zip(file_paths, hrv_features_per_user):\n",
    "        segments: pd.DataFrame = pd.read_csv(file_path).drop(\"Unnamed: 0\", axis=1)\n",
    "        domain_segments: List[pd.DataFrame] = split_domains(segments=segments)\n",
    "        domain_segments_rms: List[pd.DataFrame] = [\n",
    "            df.rolling(3, axis=1).mean() for df in domain_segments\n",
    "        ]\n",
    "\n",
    "        stat_features: pd.DataFrame = pd.concat(\n",
    "            [\n",
    "                extract_stat_features(segments=segments_rms, domain=domain)\n",
    "                for domain, segments_rms in zip(DOMAINS, domain_segments_rms)\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        gsr_features: pd.DataFrame = extract_eda_features(domain_segments_rms[GSR])\n",
    "        all_features: pd.DataFrame = pd.concat(\n",
    "            [stat_features, gsr_features, hrv_features], axis=1\n",
    "        ).drop(\"index\", axis=1)\n",
    "        all_features_per_user.append(all_features)\n",
    "\n",
    "    return all_features_per_user"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "def scale(raw_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.DataFrame(\n",
    "        MinMaxScaler().fit_transform(raw_features), columns=raw_features.columns\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "def standardize(raw_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.DataFrame(\n",
    "        StandardScaler().fit_transform(raw_features), columns=raw_features.columns\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "def merge_features(\n",
    "    features_per_user: List[pd.DataFrame],\n",
    ") -> [pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    raw_features: pd.DataFrame = pd.concat(\n",
    "        [raw_f for raw_f in features_per_user], ignore_index=True\n",
    "    )\n",
    "    scaled_features: pd.DataFrame = pd.concat(\n",
    "        [scale(raw_features=raw_f) for raw_f in features_per_user], ignore_index=True\n",
    "    )\n",
    "    standardized_features: pd.DataFrame = pd.concat(\n",
    "        [standardize(raw_features=raw_f) for raw_f in features_per_user],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    return raw_features, scaled_features, standardized_features\n",
    "\n",
    "\n",
    "def merge_labels(source_data_dir: str) -> pd.DataFrame:\n",
    "    return pd.concat(\n",
    "        [\n",
    "            pd.read_csv(f\"{source_data_dir}/{file_name}\")\n",
    "            for file_name in sorted(os.listdir(source_data_dir))\n",
    "            if \"labels\" in file_name\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    ).drop(\"Unnamed: 0\", axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "raw_features_per_user: List[pd.DataFrame] = extract_features(\n",
    "    source_data_dir=CLEAN_DATA_DIR\n",
    ")\n",
    "\n",
    "feature_dfs: [pd.DataFrame, pd.DataFrame, pd.DataFrame] = merge_features(\n",
    "    features_per_user=raw_features_per_user\n",
    ")\n",
    "\n",
    "save_names: [str, str, str] = (\"features_raw\", \"features_0-1\", \"features_std\")\n",
    "\n",
    "for feature_df, save_name in zip(feature_dfs, save_names):\n",
    "    feature_df.to_csv(f\"{FINAL_DATA_DIR}/{save_name}.csv\")\n",
    "\n",
    "label_df: pd.DataFrame = merge_labels(source_data_dir=CLEAN_DATA_DIR)\n",
    "label_df.to_csv(f\"{FINAL_DATA_DIR}/labels.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "def data_equal(comparee_dir: str, standard_dir: str) -> bool:\n",
    "    # Check if the directory is compared with itself\n",
    "    if comparee_dir == standard_dir:\n",
    "        return True\n",
    "\n",
    "    # Check if comparee data is equivalent\n",
    "    comparee_raw_features: pd.DataFrame = pd.read_csv(\n",
    "        f\"{comparee_dir}/features_raw.csv\", index_col=0\n",
    "    )\n",
    "    comparee_scaled_features: pd.DataFrame = pd.read_csv(\n",
    "        f\"{comparee_dir}/features_0-1.csv\", index_col=0\n",
    "    )\n",
    "    comparee_standardized_features: pd.DataFrame = pd.read_csv(\n",
    "        f\"{comparee_dir}/features_std.csv\", index_col=0\n",
    "    )\n",
    "\n",
    "    comparee_labels: pd.DataFrame = pd.read_csv(\n",
    "        f\"{comparee_dir}/labels.csv\", index_col=0\n",
    "    )\n",
    "\n",
    "    if not (\n",
    "        comparee_raw_features.shape\n",
    "        == comparee_scaled_features.shape\n",
    "        == comparee_standardized_features.shape\n",
    "    ):\n",
    "        return False\n",
    "\n",
    "    # Check if both sources have data of the same shape\n",
    "    comparee_data = pd.concat([comparee_labels, comparee_raw_features], axis=1)\n",
    "\n",
    "    standard_features: pd.DataFrame = pd.read_csv(\n",
    "        f\"{standard_dir}/features.csv\", index_col=0\n",
    "    )\n",
    "    standard_labels: pd.DataFrame = pd.read_csv(\n",
    "        f\"{standard_dir}/labels.csv\", index_col=0\n",
    "    )\n",
    "\n",
    "    standard_data = pd.concat([standard_labels, standard_features], axis=1)\n",
    "\n",
    "    if comparee_data.shape != standard_data.shape:\n",
    "        print(comparee_data.shape, standard_data.shape)\n",
    "        # return False\n",
    "\n",
    "    # Check if both sources contain the same rows\n",
    "    merged_data: pd.DataFrame = pd.merge(\n",
    "        comparee_data,\n",
    "        standard_data,\n",
    "        how=\"outer\",\n",
    "        left_index=False,\n",
    "        right_index=False,\n",
    "        indicator=True,\n",
    "    )\n",
    "    data_sources: pd.Index = (\n",
    "        merged_data[\"_merge\"].unique().remove_unused_categories().categories\n",
    "    )\n",
    "\n",
    "    if len(data_sources) > 1 or \"both\" not in data_sources:\n",
    "        return False\n",
    "\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(837, 127) (838, 127)\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_equal(comparee_dir=FINAL_DATA_DIR, standard_dir=\"end_data\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
